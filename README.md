# ğŸ¹ Mocktailverse: AWS ETL Pipeline

An end-to-end AWS-native ETL/ELT pipeline demonstrating enterprise-grade data engineering practices with serverless technologies.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Amazon S3     â”‚    â”‚   AWS Glue      â”‚    â”‚  DynamoDB       â”‚
â”‚   (Extract)     â”‚â”€â”€â”€â–¶â”‚   (Transform)   â”‚â”€â”€â”€â–¶â”‚   (Load)        â”‚
â”‚                 â”‚    â”‚   PySpark ETL   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS Lambda    â”‚    â”‚ Apache Airflow  â”‚    â”‚     dbt-core    â”‚
â”‚ (Enrichment)    â”‚    â”‚ (Orchestration) â”‚    â”‚   (Modeling)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- AWS CLI configured with appropriate permissions
- Docker (optional, for containerized deployment)
- Python 3.11+

### 1. Clone and Setup
```bash
git clone <repository-url>
cd mocktailverse

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure AWS Credentials
```bash
aws configure
# Enter your AWS Access Key ID, Secret Access Key, and default region
```

### 3. Deploy Infrastructure
```bash
# Create S3 buckets, DynamoDB tables, and IAM roles
# (Infrastructure as Code deployment scripts would go here)
```

### 4. Run ETL Pipeline
```bash
# Trigger the Airflow DAG
airflow dags trigger mocktailverse_etl_pipeline
```

## ğŸ“ Project Structure

```
mocktailverse/
â”œâ”€â”€ airflow_dag.py              # Apache Airflow ETL orchestration
â”œâ”€â”€ lambda/
â”‚   â””â”€â”€ transform.py            # AWS Lambda data enrichment function
â”œâ”€â”€ glue_job.py                 # AWS Glue PySpark ETL job
â”œâ”€â”€ dbt_project/                # dbt data transformation project
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/
â”‚       â””â”€â”€ marts/
â”œâ”€â”€ dynamodb_schema.json        # DynamoDB table schema
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Container configuration
â”œâ”€â”€ docker-entrypoint.sh        # Docker entrypoint script
â””â”€â”€ README.md                   # This file
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Extract** | Amazon S3 | Raw data storage and ingestion |
| **Transform** | AWS Glue + PySpark | Distributed data processing |
| **Transform** | AWS Lambda | Serverless data enrichment |
| **Load** | Amazon DynamoDB | NoSQL data storage |
| **Model** | dbt-core + Athena | Data modeling and analytics |
| **Orchestrate** | Apache Airflow | Pipeline orchestration |

## ğŸ¯ Key Features

### Enterprise ETL Capabilities
- **Serverless Architecture**: No servers to manage, automatic scaling
- **Distributed Processing**: PySpark on AWS Glue for large datasets
- **Data Quality**: Automated validation and monitoring
- **Incremental Processing**: Efficient handling of new data
- **Error Handling**: Robust retry mechanisms and dead letter queues

### AWS Free Tier Compliant
- **Lambda**: 1M requests/month free
- **Glue**: 1 DPU-hour/month free for first 1M objects
- **S3**: 5GB storage + 20K GET requests free
- **DynamoDB**: 25GB storage + 200M requests free
- **Athena**: Pay per query (typically very low cost)

## ğŸ“Š Data Flow

1. **Extract**: Cocktail data ingested from APIs â†’ Stored in S3
2. **Transform**: PySpark ETL on Glue â†’ Data cleansing and standardization
3. **Enrich**: Lambda functions â†’ Add metadata and derived fields
4. **Load**: Processed data â†’ DynamoDB for operational queries
5. **Model**: dbt transforms â†’ Analytics-ready tables in Athena

## ğŸ”§ Configuration

### Environment Variables
```bash
# AWS Configuration
AWS_DEFAULT_REGION=us-east-1
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key

# Airflow Configuration
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
```

### AWS Permissions Required
- `s3:GetObject`, `s3:PutObject`, `s3:ListBucket`
- `dynamodb:GetItem`, `dynamodb:PutItem`, `dynamodb:Query`
- `lambda:InvokeFunction`
- `glue:StartJobRun`
- `athena:StartQueryExecution`

## ğŸ“ˆ Monitoring & Observability

- **Airflow UI**: Pipeline status and task monitoring
- **CloudWatch**: AWS service metrics and logs
- **Athena Queries**: Data quality and analytics
- **DynamoDB Metrics**: Performance monitoring

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/

# Validate dbt models
cd dbt_project && dbt test

# Run ETL pipeline locally
airflow dags test mocktailverse_etl_pipeline
```

## ğŸš€ Deployment

### Docker Deployment
```bash
# Build and run with Docker
docker build -t mocktailverse-etl .
docker run -p 8080:8080 mocktailverse-etl airflow webserver
```

### AWS Deployment
- Deploy Lambda functions via AWS SAM/CloudFormation
- Create Glue jobs through AWS Console or CDK
- Set up Airflow on EC2/ECS or MWAA (Managed Workflows)

## ğŸ“š Documentation

- [AWS Glue PySpark ETL](./glue_job.py) - Distributed data processing
- [AWS Lambda Enrichment](./lambda/transform.py) - Serverless transformations
- [Airflow Orchestration](./airflow_dag.py) - Pipeline scheduling
- [dbt Data Modeling](./dbt_project/) - Analytics engineering

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes with proper tests
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## âš ï¸ Cost Considerations

This pipeline is designed to operate within AWS Free Tier limits for development and small-scale production. For larger workloads:

- Monitor Glue DPU-hours usage
- Consider DynamoDB on-demand pricing for variable loads
- Use Athena for cost-effective analytics queries
- Implement data lifecycle policies for S3

## ğŸ†˜ Support

For issues and questions:
1. Check the troubleshooting guide
2. Review AWS service documentation
3. Open an issue on GitHub

---

**Built with â¤ï¸ for the AWS Data Engineering community**